<div align="center">

<img src="assets/header.svg" alt="PersonaLive" width="100%">

<h2>Expressive Portrait Image Animation for Live Streaming</h2>

#### [Zhiyuan Li<sup>1,2,3</sup>](https://huai-chang.github.io/) Â· [Chi-Man Pun<sup>1</sup>](https://cmpun.github.io/) ğŸ“ª Â· [Chen Fang<sup>2</sup>](http://fangchen.org/) Â· [Jue Wang<sup>2</sup>](https://scholar.google.com/citations?user=Bt4uDWMAAAAJ&hl=en) Â· [Xiaodong Cun<sup>3</sup>](https://vinthony.github.io/academic/) ğŸ“ª
<sup>1</sup> University of Macau  &nbsp;&nbsp; <sup>2</sup> [Dzine.ai](https://www.dzine.ai/)  &nbsp;&nbsp; <sup>3</sup> [GVC Lab, Great Bay University](https://gvclab.github.io/)

<a href='https://arxiv.org/abs/2512.11253'><img src='https://img.shields.io/badge/ArXiv-2512.11253-red'></a> <a href='https://huggingface.co/huaichang/PersonaLive'><img src='https://img.shields.io/badge/%F0%9F%A4%97%20Hugging%20Face-Model-ffc107'></a> [![GitHub](https://img.shields.io/github/stars/GVCLab/PersonaLive?style=social)](https://github.com/GVCLab/PersonaLive)

<img src="assets/highlight.svg" alt="highlight" width="95%">

<img src="assets/demo_3.gif" width="46%"> &nbsp;&nbsp; <img src="assets/demo_2.gif" width="40.5%">
</div>

## ğŸ“£ Updates
- **[2025.12.15]** ğŸ”¥ Release `paper`!
- **[2025.12.12]** ğŸ”¥ Release `inference code`, `config` and `pretrained weights`!

## âš™ï¸ Framework
<img src="assets/overview.png" alt="Image 1" width="100%">


We present PersonaLive, a `real-time` and `streamable` diffusion framework capable of generating `infinite-length` portrait animations on a single `12GB GPU`.


## ğŸš€ Getting Started
### ğŸ›  Installation
```
# clone this repo
git clone https://github.com/GVCLab/PersonaLive
cd PersonaLive

# Create conda environment
conda create -n personalive python=3.10
conda activate personalive

# Install packages with pip
pip install -r requirements.txt
```

### â¬ Download weights
1. Download pre-trained weight of based models and other components ([sd-image-variations-diffusers](https://huggingface.co/lambdalabs/sd-image-variations-diffusers) and [sd-vae-ft-mse](https://huggingface.co/stabilityai/sd-vae-ft-mse))
    ```
    # You can run the following command to download weights automatically:
    python tools/download_weights.py
    ```

2. Download pre-trained weights into the `./pretrained_weights` folder.
    
    <a href='https://drive.google.com/drive/folders/1GOhDBKIeowkMpBnKhGB8jgEhJt_--vbT?usp=drive_link'><img src='https://img.shields.io/badge/Google%20Drive-5B8DEF?style=for-the-badge&logo=googledrive&logoColor=white'></a> <a href='https://pan.baidu.com/s/1DCv4NvUy_z7Gj2xCGqRMkQ?pwd=gj64'><img src='https://img.shields.io/badge/Baidu%20Netdisk-3E4A89?style=for-the-badge&logo=baidu&logoColor=white'></a> <a href='https://www.alipan.com/s/jyJ9JBqS6Ty'><img src='https://img.shields.io/badge/Aliyun%20Drive-E67E22?style=for-the-badge&logo=alibabacloud&logoColor=white'></a> <a href='https://huggingface.co/huaichang/PersonaLive'><img src='https://img.shields.io/badge/HuggingFace-C8AC50?style=for-the-badge&logo=huggingface&logoColor=white'></a>

Finally, these weights should be orgnized as follows:
```
pretrained_weights
â”œâ”€â”€ onnx
â”‚   â”œâ”€â”€ unet_opt
â”‚   â”‚   â”œâ”€â”€ unet_opt.onnx
â”‚   â”‚   â””â”€â”€ unet_opt.onnx.data
â”‚   â””â”€â”€ unet
â”œâ”€â”€ personalive
â”‚   â”œâ”€â”€ denoising_unet.pth
â”‚   â”œâ”€â”€ motion_encoder.pth
â”‚   â”œâ”€â”€ motion_extractor.pth
â”‚   â”œâ”€â”€ pose_guider.pth
â”‚   â”œâ”€â”€ reference_unet.pth
â”‚   â””â”€â”€ temporal_module.pth
â”œâ”€â”€ sd-vae-ft-mse
â”‚   â”œâ”€â”€ diffusion_pytorch_model.bin
â”‚   â””â”€â”€ config.json
â””â”€â”€ sd-image-variations-diffusers
â”‚   â”œâ”€â”€ image_encoder
â”‚   â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â”‚   â””â”€â”€ config.json
â”‚   â”œâ”€â”€ unet
â”‚   â”‚   â”œâ”€â”€ diffusion_pytorch_model.bin
â”‚   â”‚   â””â”€â”€ config.json
â”‚   â””â”€â”€ model_index.json
â””â”€â”€ tensorrt
    â””â”€â”€ unet_work.engine
```

### ğŸï¸ Offline Inference
```
python inference_offline.py
```
### ğŸ“¸ Online Inference
#### ğŸ“¦ Setup Web UI
```
# install Node.js 18+
curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash
nvm install 18

cd webcam
source start.sh
```

#### ğŸï¸ Acceleration (Optional)
Converting the model to TensorRT can significantly speed up inference (~ 2x âš¡ï¸). Building the engine may take about `20 minutes` depending on your device. Note that TensorRT optimizations may lead to slight variations or a small drop in output quality.
```
python torch2trt.py
```

#### â–¶ï¸ Start Streaming
```
python inference_online.py
```
then open `http://0.0.0.0:7860` in your browser. (*If `http://0.0.0.0:7860` does not work well, try `http://localhost:7860`)

<!-- ## ğŸ“‹ Citation
If you find PersonaLive useful for your research, welcome to ğŸŒŸ this repo and cite our work using the following BibTeX:
```bibtex

``` -->

## â¤ï¸ Acknowledgement
This code is mainly built upon [Moore-AnimateAnyone](https://github.com/MooreThreads/Moore-AnimateAnyone), [X-NeMo](https://byteaigc.github.io/X-Portrait2/), [StreamDiffusion](https://github.com/cumulo-autumn/StreamDiffusion), [RAIN](https://pscgylotti.github.io/pages/RAIN/) and [LivePortrait](https://github.com/KlingTeam/LivePortrait), thanks to their invaluable contributions.
